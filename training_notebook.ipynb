{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPUaopAegk0"
      },
      "source": [
        "# **Importings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsKnHCnejUj"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYq2_T2lQt-Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy import signal\n",
        "import scipy.io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_E6WPEGV6ye"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU Runtime Detected\")\n",
        "\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"No GPU Found - CPU Runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vpl4yE4eon8"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZmkdpYldZT"
      },
      "source": [
        "### Faulty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZcF4KaClbfk"
      },
      "outputs": [],
      "source": [
        "# declare the base_dir to the upper folder of the dataset with the following directories\n",
        "base_dir = ''\n",
        "os.listdir(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhwk2wablkaS"
      },
      "outputs": [],
      "source": [
        "DE_12_dir = base_dir + '12DriveEndFault/'\n",
        "loads = os.listdir(DE_12_dir)\n",
        "loads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzVGePgHlbdz"
      },
      "outputs": [],
      "source": [
        "for load in loads:\n",
        "  files = os.listdir(base_dir + '12DriveEndFault/' + load + '/')\n",
        "  print(load, ': ', len(files), ' - ', files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZQRFm-klmab"
      },
      "outputs": [],
      "source": [
        "def signal_divider(signal, window_len):\n",
        "\n",
        "    signal = np.array(signal)\n",
        "\n",
        "    fractions_of_signal = []\n",
        "    starting_index = 0\n",
        "    while starting_index < len(signal):\n",
        "\n",
        "        fractions_of_signal.append(signal[starting_index : starting_index + window_len])\n",
        "        starting_index = starting_index + window_len\n",
        "\n",
        "    if len(fractions_of_signal[-1]) != window_len:\n",
        "      del fractions_of_signal[-1]\n",
        "\n",
        "    return pd.DataFrame(np.array(fractions_of_signal).reshape(len(fractions_of_signal), window_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA9K9TY1lmX2"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "DE_dict = {}\n",
        "for load in loads:\n",
        "  for file in os.listdir(base_dir + '12DriveEndFault/' + load + '/'):\n",
        "\n",
        "    file = \"\".join(list(file)[:-4])\n",
        "    mat_data = scipy.io.loadmat(base_dir + '12DriveEndFault/' + load + '/' + file)\n",
        "\n",
        "    for key in list(mat_data.keys()):\n",
        "      if key.startswith(\"X\"):\n",
        "        code = key[:4]\n",
        "\n",
        "    if code + '_DE_time' in mat_data:\n",
        "      DE_dict[load + '-' + file] = mat_data[code + '_DE_time']\n",
        "\n",
        "    print(i, ':  - ' , load + '-' + file , mat_data.keys())\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOAUL-C2lmVb"
      },
      "outputs": [],
      "source": [
        "window_len = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDbX4lbilmS8"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "for key in list(DE_dict.keys()):\n",
        "  load, sev, state = key.split('-')\n",
        "  print(key, '\\n')\n",
        "  print('Load: ', load, ' - Severity: ', sev, ' - State: ', state, 'Len: ', np.shape(DE_dict[key]), '\\n\\n')\n",
        "  temp_df = signal_divider(DE_dict[key], window_len)\n",
        "  temp_df['load'] = load\n",
        "  temp_df['sev'] = sev\n",
        "  temp_df['state'] = state\n",
        "  dfs.append(temp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaTKqlNclmQE"
      },
      "outputs": [],
      "source": [
        "DE_faulty_time_df = pd.concat(dfs).reset_index(drop = True)\n",
        "DE_faulty_time_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaaIwXOjlc6b"
      },
      "source": [
        "### Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThlNvARumBQ0"
      },
      "outputs": [],
      "source": [
        "normal_dir = base_dir + 'NormalBaseline/'\n",
        "loads = os.listdir(normal_dir)\n",
        "loads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQAJcshcmBN8"
      },
      "outputs": [],
      "source": [
        "for load in loads:\n",
        "  files = os.listdir(normal_dir + load + '/')\n",
        "  print(load, ': ', len(files), ' - ', files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMP9_KFvmBLT"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "DE_dict = {}\n",
        "for load in loads:\n",
        "  for file in os.listdir(normal_dir + load + '/'):\n",
        "\n",
        "    file = \"\".join(list(file)[:-4])\n",
        "    mat_data = scipy.io.loadmat(normal_dir + load + '/' + file)\n",
        "\n",
        "    for key in list(mat_data.keys()):\n",
        "      if key.startswith(\"X\"):\n",
        "        code = key[:4]\n",
        "\n",
        "    if code + '_DE_time' in mat_data:\n",
        "      DE_dict[load + '-' + file] = mat_data[code + '_DE_time']\n",
        "\n",
        "    print(i, ':  - ' , load + '-' + file , mat_data.keys())\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO2uhO5kmBI7"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "for key in list(DE_dict.keys()):\n",
        "  load, state = key.split('-')\n",
        "  print(key, '\\n')\n",
        "  print('Load: ', load, ' - State: ', state, 'Len: ', np.shape(DE_dict[key]), '\\n\\n')\n",
        "  temp_df = signal_divider(DE_dict[key], window_len)\n",
        "  temp_df['load'] = load\n",
        "  temp_df['state'] = state\n",
        "  dfs.append(temp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5OXeji6mBGO"
      },
      "outputs": [],
      "source": [
        "DE_normal_time_df = pd.concat(dfs).reset_index(drop = True)\n",
        "DE_normal_time_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8hRmsPgmBBR"
      },
      "outputs": [],
      "source": [
        "DE_normal_time_df['sev'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKAy2UlYG1G5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNdLymtHmA-k"
      },
      "outputs": [],
      "source": [
        "DE_time_df = pd.concat([DE_faulty_time_df, DE_normal_time_df]).reset_index(drop = True)\n",
        "DE_time_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLyL5LKolyt8"
      },
      "outputs": [],
      "source": [
        "def ff_transformer(time_signal, freq_filter_flag = False, window_flag = False):\n",
        "    N = len(time_signal)\n",
        "\n",
        "    if freq_filter_flag is True:\n",
        "      band_pass_filter = signal.butter(25, [2.5, 1500], 'bandpass', fs=3200, output='sos')\n",
        "      time_signal = signal.sosfilt(band_pass_filter, time_signal)\n",
        "\n",
        "    if window_flag is True:\n",
        "      window = signal.windows.hann(N)\n",
        "      time_signal = time_signal * window\n",
        "\n",
        "    fft_signal = 2.0/N * np.abs(fft(np.array(time_signal))[0:N//2])\n",
        "\n",
        "    return fft_signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJN23S3Rlyri"
      },
      "outputs": [],
      "source": [
        "def df_ff_transformer(time_df):\n",
        "    freq_domain = []\n",
        "    for index, row in time_df.iterrows():\n",
        "        freq_domain.append(ff_transformer(row, freq_filter_flag = True, window_flag = True))\n",
        "\n",
        "    freq_domain_df = pd.DataFrame(freq_domain)\n",
        "    return freq_domain_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlLAHLbnje3"
      },
      "source": [
        "# **Data Preprocessing and Preparation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnpLQ-8SmA77"
      },
      "outputs": [],
      "source": [
        "DE_time_df['state'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q4zQHoorvu-"
      },
      "source": [
        "## Aggregating all the OuterRaces to Single One"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Zuy0_jucKG"
      },
      "source": [
        "In this step, all the variants of **OuterRace** problem, including *OuterRace3*, *OuterRace6* and *OuterRace12* are aggregate to form a single **OuterRace** Health State."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-D8JFNNrH-F"
      },
      "outputs": [],
      "source": [
        "for i in range(len(DE_time_df['state'])):\n",
        "  if DE_time_df['state'][i].startswith('O'):\n",
        "    DE_time_df['state'][i] = 'OuterRace'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRm_CdAJo-9v"
      },
      "outputs": [],
      "source": [
        "DE_time_df['state'] = DE_time_df['state'].astype('category')\n",
        "DE_time_df['state'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx5BpQt6sAJX"
      },
      "source": [
        "## Encoding State to Numerical Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyGZxTxSo-7O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder= LabelEncoder()\n",
        "DE_time_df['state_encoded'] = labelencoder.fit_transform(DE_time_df['state'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA_WhVhvo-4w"
      },
      "outputs": [],
      "source": [
        "DE_time_df['state_encoded'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlA59cfOsUru"
      },
      "source": [
        "## Train/Test Spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7upszCVds54"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6NFX4n4sXSm"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(DE_time_df, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "EZT1Anwsi7Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exporting_dir = '/content/drive/My Drive/brand_new_CWRU/basic_classification/12DE_fault/exported_models/'\n",
        "df_test.groupby(['state', 'load', 'sev']).sample(5).reset_index(drop = True).to_csv(exporting_dir + 'subsampled_test_df.csv')"
      ],
      "metadata": {
        "id": "mtj_hR7Qj7eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lkmI9zUsltd"
      },
      "outputs": [],
      "source": [
        "x_train = df_train.iloc[:, :2048]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "x_test = df_test.iloc[:, :2048]\n",
        "y_test = df_test.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIqxRpUsu9k3"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVwgjtbdvAgP"
      },
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OYmVErhtM9P"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler(arr):\n",
        "\n",
        "  mu = arr.float().mean(dim = 1)\n",
        "  sig = arr.float().std(dim = 1)\n",
        "\n",
        "  array_scaled = (torch.subtract(arr.transpose(0, 1), mu) / sig).transpose(0, 1)\n",
        "\n",
        "  return array_scaled"
      ],
      "metadata": {
        "id": "q4-RDPrJFCLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHBgL8PyV1x"
      },
      "source": [
        "# **Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNFGyQL8yY6c"
      },
      "source": [
        "## Function - Model Creator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5GSpTtQOp22"
      },
      "outputs": [],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels = 1, out_channels = 5, kernel_size = 100)\n",
        "        self.avgPool = nn.AvgPool1d(kernel_size = 50)\n",
        "        self.fc = nn.Linear(5 * 38, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        z = self.conv(x)\n",
        "        z = torch.tanh(z)\n",
        "        z = self.avgPool(z)\n",
        "\n",
        "        z = z.view(-1, 5 * 38)\n",
        "\n",
        "        z = self.fc(z)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li_QErilYMhw"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = Classifier().to(device)\n",
        "summary(model, (1, 2048))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview"
      ],
      "metadata": {
        "id": "nwQI7_S4zPnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchview import draw_graph\n",
        "model_graph = draw_graph(model, input_size=(1, 2048), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "diTdF7LMy93Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzA_ILU5yjfR"
      },
      "source": [
        "## Creating, Compiling and Fitting the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A73VcP8JjEhQ"
      },
      "outputs": [],
      "source": [
        "x_train_train, x_train_validation, y_train_train, y_train_validation = train_test_split(x_train, y_train, test_size = 0.25)\n",
        "\n",
        "# x_train_train = x_train_train.to_numpy().reshape(2292, 1, 2048)\n",
        "# x_train_validation = x_train_validation.to_numpy().reshape(764, 1, 2048)\n",
        "\n",
        "x_train_train_scaled = scaler(torch.Tensor(x_train_train.to_numpy()).float()).reshape(2292, 1, 2048)\n",
        "x_train_validation_scaled = scaler(torch.Tensor(x_train_validation.to_numpy()).float()).reshape(764, 1, 2048)\n",
        "\n",
        "x_train_VAR = torch.autograd.Variable(x_train_train_scaled).to(device)\n",
        "y_train_VAR = torch.autograd.Variable(torch.LongTensor(y_train_train.to_numpy())).to(device)\n",
        "x_valid_VAR = torch.autograd.Variable(torch.Tensor(x_train_validation_scaled).float()).to(device)\n",
        "y_valid_VAR = torch.autograd.Variable(torch.LongTensor(y_train_validation.to_numpy())).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1U1yiFrO_cN"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "ep = 5000\n",
        "\n",
        "model = Classifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr = lr,\n",
        "                             weight_decay = lr / ep)\n",
        "\n",
        "losses = []\n",
        "valid_losses = []\n",
        "accs = []\n",
        "valid_accs = []\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(ep):\n",
        "\n",
        "  # validation step\n",
        "  valid_loss = criterion(model(x_valid_VAR), y_valid_VAR).item()\n",
        "  valid_losses.append(valid_loss)\n",
        "  valid_acc = accuracy_score(y_train_validation, np.argmax(model(x_valid_VAR).cpu().detach().numpy(), axis = 1))\n",
        "  valid_accs.append(valid_acc)\n",
        "\n",
        "  # training step\n",
        "  optimizer.zero_grad()\n",
        "  loss = criterion(model(x_train_VAR), y_train_VAR)\n",
        "  acc = accuracy_score(y_train_train, np.argmax(model(x_train_VAR).cpu().detach().numpy(), axis = 1))\n",
        "  accs.append(acc)\n",
        "  losses.append(loss.item())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Epoch {epoch+1}, loss: {np.round(loss.item(), 4)}  , Vloss: {np.round(valid_loss, 4)}, acc: {np.round(acc, 4)}, Vacc: {np.round(valid_acc, 4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbWar3HRrns4"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16,4))\n",
        "fig.suptitle('Deep Learning Model Training Process')\n",
        "axes[0].plot(losses, label='Training Loss')\n",
        "axes[0].plot(valid_losses, label='Validation Loss')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss Vs. Epochs')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(accs, label='Training Accuracy')\n",
        "axes[1].plot(valid_accs, label='Validation Accuracy')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Accuracy Vs. Epochs')\n",
        "axes[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDj6S96jyUJL"
      },
      "source": [
        "# **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a97dzEmZSHOt"
      },
      "outputs": [],
      "source": [
        "x_test_scaled = scaler(torch.Tensor(x_test.to_numpy()))\n",
        "x_test_VAR = torch.autograd.Variable(x_test_scaled).reshape(1311, 1, 2048).to(device)\n",
        "testing_acc = accuracy_score(y_test, np.argmax(F.softmax(model(x_test_VAR)).cpu().detach().numpy(), axis = 1))\n",
        "print('Testing Accuracy:   ', testing_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loTnxIYuw6PZ"
      },
      "outputs": [],
      "source": [
        "y_test_pred = np.argmax(F.softmax(model(x_test_VAR)).cpu().detach().numpy(), axis = 1)\n",
        "y_test_pred_decoded = labelencoder.inverse_transform(y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b43J_YwmY9CX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "ax = sns.heatmap(matrix, annot=True, fmt='d', cbar = True, square = True, cmap = 'Blues')\n",
        "ax.set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\n",
        "ax.xaxis.set_ticklabels(np.unique(y_test_pred_decoded))\n",
        "ax.set_ylabel(\"Actual\", fontsize=14, labelpad=20)\n",
        "ax.yaxis.set_ticklabels(np.unique(y_test_pred_decoded))\n",
        "ax.set_title(\"Confusion Matrix\", fontsize=14, pad=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXzYfOVEo-Fn"
      },
      "outputs": [],
      "source": [
        "# delcare the exporting_dir as the directory you want to export the trained model\n",
        "exporting_dir = ''\n",
        "torch.save(model.state_dict(), exporting_dir + 'lightCNN_timeClassifier_Pytorch_preprocessing_state_dict.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaRxy4z7o-CX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}